{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import meteomatics.api as api\n",
    "import datetime as dt\n",
    "import shapely.geometry as sgeometry\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import geopandas as gpd\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import functools\n",
    "import glob\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINIÇÕES DE DIRETÓRIOS\n",
    "BASE_DIR = os.getcwd()\n",
    "DIR_DADOS = os.path.join(BASE_DIR, \"dados\")\n",
    "DIR_COUNTRIES = os.path.join(DIR_DADOS, \"ne_110m_admin_0_countries\")\n",
    "DIR_STATES = os.path.join(DIR_DADOS, \"ne_110m_admin_1_states_provinces\")\n",
    "countries_shp_path = os.path.join(DIR_COUNTRIES, \"ne_110m_admin_0_countries.shp\")\n",
    "states_shp_path = os.path.join(DIR_STATES, \"ne_110m_admin_1_states_provinces.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o shapefile\n",
    "countries = gpd.read_file(countries_shp_path)\n",
    "paises = countries['NAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29    POLYGON ((-53.37366 -33.76838, -53.65054 -33.2...\n",
       "Name: geometry, dtype: geometry"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brasil = countries[countries['NAME'] == 'Brazil']\n",
    "brasil['geometry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29    POLYGON ((-53.37366 -33.76838, -53.65054 -33.2...\n",
       "Name: geometry, dtype: geometry"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geometria = brasil['geometry']\n",
    "geometria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas = brasil.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  CONFIGURATION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. What countries/regions do you want in your time-series?\n",
    "# These are provided in two arguments. 'STANDARD_COUNTRIES' is a list of strings which are the names of polygons in\n",
    "# geopandas' \"naturalearth_lowres\" built-in dataset. 'MANUAL_GEOMETRIES' is a dictionary whose keys are the names you\n",
    "# want to give to your user-created polygons, and whose keys are the paths to geojson files containing those polygons.\n",
    "# I advise that, if you make changes to this, you check that they have been interpreted correctly by switching\n",
    "# 'SHOW_POLYGONS' to True. This will show them in your chosen map projection. Speaking of which...\n",
    "STANDARD_COUNTRIES = ['United Kingdom', 'Germany', 'Netherlands', 'Italy']\n",
    "MANUAL_GEOMETRIES = {'Northern France': 'polygons/northern_france.geojson'}\n",
    "SHOW_POLYGONS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. What map projection do you want to show your animated data in?\n",
    "# Change entries in the dictionaries to change the bounding box for your animations. This will make a rectangular plot\n",
    "# regardless of the projection. The dictionary values are then used to define the map projection. You can choose from\n",
    "# any cartopy projection, but be aware that a) some projections take different arguments to define the transformations\n",
    "# and b) some projections will not fill the rectangle defined by the dictionaries in certain parts of the globe (for\n",
    "# instance, if you use AzimuthalEquidistant in Europe you'll see the fetched data get squeezed at the top of the plot).\n",
    "TOP_LEFT = {\n",
    "    'lat': 66.211199,\n",
    "    'lon': -32.364446\n",
    "}\n",
    "BOTTOM_RIGHT = {\n",
    "    'lat': 35.526388,\n",
    "    'lon': 32\n",
    "}\n",
    "# CRS = ccrs.AzimuthalEquidistant(central_longitude=(TOP_LEFT['lon']+BOTTOM_RIGHT['lon'])/2,\n",
    "#                                 central_latitude=(TOP_LEFT['lat']+BOTTOM_RIGHT['lat'])/2)\n",
    "CRS = ccrs.Mercator(central_longitude=(TOP_LEFT['lon']+BOTTOM_RIGHT['lon'])/2,\n",
    "                    min_latitude=BOTTOM_RIGHT['lat'],\n",
    "                    max_latitude=TOP_LEFT['lat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. What variables are you interested in?\n",
    "# Currently this choice only affects time-series plots: the variables used in the animation are static (temperature and\n",
    "# pressure, see animate <== make_nc). Keys should be the names you want on your axis labels, and should feature exactly\n",
    "# one set of parentheses enclosing the units; values are the names of the corresponding meteomatics strings.\n",
    "TIMESERIES_VARS = {\n",
    "    'Temperature (C)': 't_2m:C',\n",
    "    '90m Wind Speed (m/s)': 'wind_speed_90m:ms',\n",
    "    'Global Radiation (W/m2)': 'global_rad:W'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. How do you want to define the baseline (climatology) for your time-series?\n",
    "# 15 years is apparently standard for energy industry; 30 years is standard in academic texts. Climatologies are all\n",
    "# interpolated to 1hrly resolution, but I recommend sub-6hrly for data acquisition, since 0600 and 1800 vary between\n",
    "# being daytime- and nighttime values throughout the year. Also note that the API assumes all times are UTC, so a\n",
    "# smaller time-step is better for translating to other time-zones). Don't let your climatology get too out of date!\n",
    "CLIMATOLOGY_START_YEAR = 2005\n",
    "CLIMATOLOGY_STOP_YEAR = 2020\n",
    "CLIMATOLOGY_STEP = dt.timedelta(hours=3)\n",
    "\n",
    "# IMPORTANT! Climatologies are obtained once per new run environment and then imported from .csv in order to reduce\n",
    "# runtime. The script is not clever enough to know whether you have changed TIMESERIES_VARS or the period to be covered\n",
    "# by climatology, so make sure you force the recalculation of climatology whenever changes are made.\n",
    "REBUILD_CLIMATOLOGY = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Plot formatting.\n",
    "#   What period do you want your output (time-series and animations) to cover?\n",
    "#   What temporal resolution do you want for them both (set separately)?\n",
    "#   What spatial resolution do you want in your\n",
    "# Note that the animation may fail if you set the time-step very high\n",
    "LEAD_TIME_DAYS = 28\n",
    "START_TIME = dt.datetime.combine(dt.datetime.now().date(), dt.time(12))  # starts at 12pm on the day called\n",
    "T_SERIES_LEAD = dt.timedelta(days=LEAD_TIME_DAYS)\n",
    "T_SERIES_STEP = dt.timedelta(hours=1)\n",
    "ANIMATION_STEP = dt.timedelta(hours=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API access\n",
    "USERNAME = 'energy_market_plots'\n",
    "PASSWORD = 'GkQ9qqq868Lm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(func):\n",
    "    \"\"\"Print the runtime of the decorated function\"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper_timer(*args, **kwargs):\n",
    "        t_start_time = time.perf_counter()    # 1\n",
    "        value = func(*args, **kwargs)\n",
    "        end_time = time.perf_counter()      # 2\n",
    "        run_time = end_time - t_start_time    # 3\n",
    "        print(f\"Finished {func.__name__!r} in {run_time:.4f} secs\")\n",
    "        return value\n",
    "    return wrapper_timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_geometries(geometries):  # TODO could add an option to show overlap\n",
    "    \"\"\"\n",
    "    Plots the projected polygons\n",
    "    :param geometries: shapely.[Multi]Polygon objects corresponding to polygons projected into CRS (defined in header)\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(projection=CRS)\n",
    "    ax.stock_img()\n",
    "    ax.set_extent([TOP_LEFT['lon'], BOTTOM_RIGHT['lon'], TOP_LEFT['lat'], BOTTOM_RIGHT['lat']])\n",
    "    for geom in geometries:\n",
    "        geometries[geom].plot(ax=ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geometries(standard_countries, manual_geometries, show=False):\n",
    "    \"\"\"\n",
    "    Uses geopandas' built-in countries for those polygons which correspond to a whole country; otherwise can use custom\n",
    "    geometries provided in GeoJSON format (with corresponding user-defined polygon names: see parameter docstrings).\n",
    "    :param standard_countries: list of country names as they appear in naturalearth_lowres\n",
    "    :param manual_geometries: dictionary of:\n",
    "                polygon names : GeoJSON paths\n",
    "    :param show: Bool, if True will illustrate location and shape of polygons on given map projection\n",
    "    :return: dictionary of:\n",
    "                polygon names (standard_countries + manual_geometries.keys) : shapely.[Multi]Polygon\n",
    "    \"\"\"\n",
    "    # Utilize o novo método para carregar os dados naturalearth_lowres\n",
    "    countries = gpd.read_file(countries_shp_path)\n",
    "\n",
    "    geometries = {}\n",
    "    geometries_projected = {}  # will be filled with geometries in chosen projection if show is True\n",
    "    for country in standard_countries:\n",
    "        geometries[country] = countries[countries['NAME'] == country].geometry.values[0]  # Use a coluna correta aqui\n",
    "        if show:\n",
    "            geometries_projected[country] = countries[countries['NAME'] == country].to_crs(CRS.proj4_init).geometry\n",
    "    for country in manual_geometries:\n",
    "        geometries[country] = gpd.read_file(manual_geometries[country]).geometry.values[0]\n",
    "        if show:\n",
    "            geometries_projected[country] = gpd.read_file(manual_geometries[country]).to_crs(CRS.proj4_init).geometry\n",
    "    if show:\n",
    "        show_geometries(geometries_projected)\n",
    "    return geometries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tuple_list(polygon):\n",
    "    \"\"\"\n",
    "    Subroutine of make_tuple_lists. Argument will always be a Polygon, since parent function loops over MultiPolygon\n",
    "    :param polygon: shapely.Polygon\n",
    "    :return: tuple list\n",
    "    \"\"\"\n",
    "    tuple_list = []\n",
    "    lons, lats = polygon.exterior.coords.xy\n",
    "    for i in range(len(lons)):\n",
    "        tuple_list.append((lats[i], lons[i]))\n",
    "    return tuple_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tuple_lists(geometry):\n",
    "    \"\"\"\n",
    "    Prepare the tuple list argument for API query from polygon data\n",
    "    :param geometry: shapely.[Multi]Polygon\n",
    "    :return: tuple lists (tuple list for each Polygon)\n",
    "    \"\"\"\n",
    "    retval = []\n",
    "    if type(geometry) is sgeometry.multipolygon.MultiPolygon:\n",
    "        for polygon in geometry:  # this is the correct way to access Polygons within a MultiPolygon...\n",
    "            retval.append(make_tuple_list(polygon))\n",
    "    elif type(geometry) is sgeometry.polygon.Polygon:\n",
    "        retval.append(make_tuple_list(geometry))  # ...but you can't loop over a single Polygon (which is dumb)\n",
    "    else:\n",
    "        # code should be written such that only [Multi]Polygons make it here; if imported modules change, will raise\n",
    "        raise TypeError('geometries should contain only polygons or multipolygons')\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_climatology(geometries, location):\n",
    "    \"\"\"\n",
    "    This only looks hefty because of the if/else block, plus the fact that I've extended API calls over multiple lines.\n",
    "    Makes a climatology dataframe by joining years together sequentially.\n",
    "    :param geometries: dictionary of:\n",
    "                locations : shapely.[Multi]Polygons\n",
    "    :param location: the name of the location. passed because desired for name of written .csv\n",
    "    \"\"\"\n",
    "    geometry = geometries[location]\n",
    "    tuple_lists = make_tuple_lists(geometry)\n",
    "    print('Fetching climatology for {}.'.format(location))\n",
    "    # had to break this down because of 300s timeout. 3hrly still seems reasonable and a bit faster; could go 1hrly now\n",
    "    for year in range(CLIMATOLOGY_START_YEAR, CLIMATOLOGY_STOP_YEAR):\n",
    "        if year == CLIMATOLOGY_START_YEAR:\n",
    "            df = api.query_polygon(\n",
    "                latlon_tuple_lists=tuple_lists,\n",
    "                startdate=dt.datetime(year, 1, 1),\n",
    "                enddate=dt.datetime(year, 12, 31, int(24 - CLIMATOLOGY_STEP.seconds/3600.)),\n",
    "                interval=CLIMATOLOGY_STEP,\n",
    "                parameters=TIMESERIES_VARS.values(),\n",
    "                aggregation=['mean'],\n",
    "                username=USERNAME,\n",
    "                password=PASSWORD,\n",
    "                operator='U',  # in case of MultiPolygons\n",
    "                model='ecmwf-era5',\n",
    "                polygon_sampling='adaptive_grid'\n",
    "            )\n",
    "        else:\n",
    "            df = df.append(\n",
    "                api.query_polygon(\n",
    "                    latlon_tuple_lists=tuple_lists,\n",
    "                    startdate=dt.datetime(year, 1, 1),\n",
    "                    enddate=dt.datetime(year, 12, 31, int(24 - CLIMATOLOGY_STEP.seconds/3600.)),\n",
    "                    interval=CLIMATOLOGY_STEP,\n",
    "                    parameters=TIMESERIES_VARS.values(),\n",
    "                    aggregation=['mean'],\n",
    "                    username=USERNAME,\n",
    "                    password=PASSWORD,\n",
    "                    operator='U',  # in case of MultiPolygons\n",
    "                    model='ecmwf-era5',\n",
    "                    polygon_sampling='adaptive_grid'\n",
    "                )\n",
    "            )\n",
    "    # Query returns MultiIndexed DataFrame, but outer index is polygon1 even if geometry is a MultiPolygon (because\n",
    "    # of the union operation). To make this easier to work with, I cross-section to get rid of the outer index level.\n",
    "    # To future-proof, make sure that there is only one outer index - if so, remove it.\n",
    "    assert len(df.index.get_level_values(0).unique()) == 1\n",
    "    df = df.xs(key='polygon1')\n",
    "    df = df.resample('H').interpolate(method='cubic')\n",
    "    climatology = df.groupby([df.index.dayofyear, df.index.time]).mean()\n",
    "    climatology.index.names = ['doy', 'time']\n",
    "    climatology.to_csv(os.path.join('climatologies', '{}.csv'.format(location)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def get_polygon_climatology(geometries, location):  # TODO could save climatology locally and only fetch if required\n",
    "    if os.path.exists(os.path.join('climatologies', '{}.csv'.format(location))) and not REBUILD_CLIMATOLOGY:\n",
    "        print('Climatology for {} exists: reading climatology'.format(location))\n",
    "    else:\n",
    "        print('New climatology required for {}.'.format(location))\n",
    "        os.makedirs('climatologies', exist_ok=True)\n",
    "        write_climatology(geometries, location)\n",
    "    df = pd.read_csv(os.path.join('climatologies', '{}.csv'.format(location)), index_col=[0, 1])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def get_polygon_data(geometry):  # TODO could make parameters variable\n",
    "    \"\"\"\n",
    "    Gets mean weather data for 2m temperature and 90m wind-speed (currently static) within polygon defined by geometry\n",
    "    :param geometry: shapely.[Multi]Polygon\n",
    "    :return: pandas.DataFrame of spatial mean time-series data for the polygon; time-series parameters defined in header\n",
    "    \"\"\"\n",
    "    tuple_lists = make_tuple_lists(geometry)\n",
    "    print('Fetching time-series data for the next {} days.'.format(LEAD_TIME_DAYS))\n",
    "    df = api.query_polygon(\n",
    "        latlon_tuple_lists=tuple_lists,\n",
    "        startdate=START_TIME,\n",
    "        enddate=START_TIME + T_SERIES_LEAD,\n",
    "        interval=T_SERIES_STEP,\n",
    "        parameters=TIMESERIES_VARS.values(),\n",
    "        aggregation=['mean'],\n",
    "        username=USERNAME,\n",
    "        password=PASSWORD,\n",
    "        operator='U',  # not necessary for the single polygon, but doesn't break and is necessary for multi-polygons\n",
    "        model='ecmwf-vareps',\n",
    "        polygon_sampling='adaptive_grid'\n",
    "    )\n",
    "    # Query returns MultiIndexed DataFrame, but outer index is polygon1 even if geometry is a MultiPolygon (because\n",
    "    # of the union operation). To make this easier to work with, I cross-section to get rid of the outer index level.\n",
    "    # To future-proof, make sure that there is only one outer index - if so, remove it.\n",
    "    assert len(df.index.get_level_values(0).unique()) == 1\n",
    "    df = df.xs(key='polygon1')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(fcst, clim):\n",
    "    \"\"\"\n",
    "    Joins the forecast and the corresponding climatology data together into one DataFrame.\n",
    "    :param fcst:\n",
    "    :param clim:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    relevant_clim = pd.DataFrame(index=fcst.index, columns=fcst.columns)\n",
    "    for i in range(len(fcst)):\n",
    "        tstamp = fcst.index[i]\n",
    "        relevant_clim.loc[tstamp] = clim.loc[(tstamp.dayofyear, tstamp.time().strftime('%H:%M:%S'))]\n",
    "    return pd.merge(fcst, relevant_clim, left_index=True, right_index=True, suffixes=('_forecast', '_climatology'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_data(geometries, key, mins, maxs):\n",
    "    \"\"\"\n",
    "    Reads the climatology and forecast data, combines them, writes the result to file and updates the min/max dicts.\n",
    "    :param geometries: dict of shapely.[Multi]Polygons\n",
    "    :param key: the country/region of interest\n",
    "    :param mins: dict of minimum values for variables across all regions\n",
    "    :param maxs: dict of maximum values for variables across all regions\n",
    "    :return: (updated) dicts of minimum/maximum values for variables across all regions\n",
    "    \"\"\"\n",
    "    clim = get_polygon_climatology(geometries, key)  # pass the key in order to name/lookup the file\n",
    "    fcst = get_polygon_data(geometries[key])\n",
    "    combined = combine(fcst, clim)\n",
    "    for var in fcst.columns:\n",
    "        mins[var] = min(mins[var], min(fcst[var]))\n",
    "        maxs[var] = max(maxs[var], max(fcst[var]))\n",
    "    os.makedirs('time-series', exist_ok=True)\n",
    "    combined.to_csv(os.path.join('time-series', 'time-series data {}.csv'.format(key)))\n",
    "    return mins, maxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_timeseries(title, mins, maxs):\n",
    "    \"\"\"\n",
    "    Generic function for plotting time-series data and anomaly from climatology of any variable (specified in header)\n",
    "    :param title: the name of the region being plotted: will be the title of the plot and the name of the file\n",
    "    :param mins: dictionary of global minimum values corresponding to each variable\n",
    "    :param maxs: dictionary of global maximum values corresponding to each variable\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(os.path.join('time-series', 'time-series data {}.csv'.format(title)), index_col=0, parse_dates=True)\n",
    "    for var in TIMESERIES_VARS:\n",
    "        try:\n",
    "            assert '(' in var\n",
    "        except AssertionError:\n",
    "            raise ValueError('Make sure the TIMESERIES_VARS dict is formatted correctly')\n",
    "        mm_string = TIMESERIES_VARS[var]\n",
    "        unit_index = len(var.split('(')[0])\n",
    "        vmin = mins[mm_string] - (maxs[mm_string] - mins[mm_string]) * 0.1\n",
    "        vmax = maxs[mm_string] + (maxs[mm_string] - mins[mm_string]) * 0.1\n",
    "        f, (ax1, ax2) = plt.subplots(2, sharex='col', figsize=(12, 7))\n",
    "        climatology_years = CLIMATOLOGY_STOP_YEAR - CLIMATOLOGY_START_YEAR\n",
    "        df['{}_climatology'.format(mm_string)].plot(label='{}-year mean'.format(climatology_years), ax=ax1, c='b')\n",
    "        df['{}_forecast'.format(mm_string)].plot(label='forecast', ax=ax1, ylim=[vmin, vmax], c='k')\n",
    "        pd.Series(np.zeros(len(df.index)), index=df.index).plot(ax=ax2, linestyle='dashed', c='b')\n",
    "        (df['{}_forecast'.format(mm_string)] - df['{}_climatology'.format(mm_string)]).plot(ax=ax2, c='k')\n",
    "        # plot formatting options\n",
    "        f.suptitle('{}'.format(title))\n",
    "        ax1.set_ylabel(var)\n",
    "        ax1.legend()\n",
    "        ax2.set_ylabel(var[:unit_index]+'Anomaly '+var[unit_index:])\n",
    "        ax1.grid(True)\n",
    "        ax2.grid(True)\n",
    "        f.tight_layout()\n",
    "        os.makedirs(os.path.join('time-series', '{}'.format(var.replace('/', ''))), exist_ok=True)\n",
    "        plt.savefig(os.path.join('time-series', '{}'.format(var.replace('/', '')), '{}.png'.format(title)))\n",
    "        plt.close(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ timer\n",
    "def make_nc():\n",
    "    \"\"\"\n",
    "    A single API call to retrieve all this data is too big, so I loop over days in our forecast range and stack the\n",
    "    grids together. Using query_grid_timeseries is still desirable for this, since it can be manipulated into an\n",
    "    xarray.Dataset very easily.\n",
    "    :return: xarray.Dataset containing 2m temperature and MSLP\n",
    "    \"\"\"\n",
    "    for day in range(LEAD_TIME_DAYS):\n",
    "        query_start = START_TIME + dt.timedelta(days=day)\n",
    "        df = api.query_grid_timeseries(\n",
    "            startdate=query_start,\n",
    "            enddate=query_start + dt.timedelta(hours=23), # ensures we don't double-count days\n",
    "            interval=ANIMATION_STEP,\n",
    "            parameters=['t_2m:C', 'msl_pressure:hPa'],\n",
    "            lat_N=np.ceil(TOP_LEFT['lat'] + 1),\n",
    "            lon_W=np.floor(TOP_LEFT['lon'] - 1),\n",
    "            lat_S=np.floor(BOTTOM_RIGHT['lat'] - 1),\n",
    "            lon_E=np.ceil(BOTTOM_RIGHT['lon'] + 1),\n",
    "            res_lon=0.5,\n",
    "            res_lat=0.5,\n",
    "            username=USERNAME,\n",
    "            password=PASSWORD,\n",
    "            model='ecmwf-vareps'\n",
    "        )\n",
    "        if day == 0:\n",
    "            nc = df.to_xarray()\n",
    "        else:\n",
    "            tmp = df.to_xarray()\n",
    "            nc = xr.concat([nc, tmp], dim='validdate')\n",
    "    try:\n",
    "        # TODO this currently fails because Meteomatics data is float64 and to_netcdf wants float32\n",
    "        nc.to_netcdf('run_{}'.format(dt.datetime.now().strftime('%Y-%m-%d_%Hh00.nc')))\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_coords(nc):\n",
    "    \"\"\"\n",
    "    I found the suggested method of adding a transorm=CRS keyword argument to plotting functions not to work with\n",
    "    e.g. Azimuthal equidistant. Perhaps this is something to do with pcolor. Anyway, here's my solution: define a\n",
    "    2D latitude and longitude variable which corresponds to the data variables; transform those and return them.\n",
    "    :param nc: xarray dataset containing latitude and longitude coordinates\n",
    "    :return: longitude and latitudes transformed to your chosen CRS\n",
    "    \"\"\"\n",
    "    # We first need latitude- and longitude arrays of equal size. Since we may be working with a map which has different\n",
    "    # x- and y-dimensions, and also since we may not be mapping to a rectilinear coordinate system, we first have to\n",
    "    # make 2D arrays of each\n",
    "    meshed_lon, meshed_lat = np.meshgrid(nc.lon.values, nc.lat.values)\n",
    "    # We can then transform these to our target CRS. The source CRS is PlateCarree() i.e. PlateCarree with default args\n",
    "    # i.e. quadratic grid, as this is the coordinate system returned from the Meteomatics API.\n",
    "    transformed_output = CRS.transform_points(ccrs.PlateCarree(), meshed_lon, meshed_lat)\n",
    "    # This gives us a 3D array of x, y, z; the latter will, for our purposes, be identically 0. We can subset this as\n",
    "    transformed_lons = transformed_output[:, :, 0]\n",
    "    transformed_lats = transformed_output[:, :, 1]\n",
    "    # Note that these are again 2D (because the same latitudes are not used for all longitudes and vice versa in a\n",
    "    # non-rectangular projection) and hence cannot be used as coordinates in an xarray.Dataset, but can be added as\n",
    "    # additional data variables if you like.\n",
    "    # This produces lat/lon fields for each element of the weather variables which allow them to be plotted in this\n",
    "    # projection. The process needn't be repeated for each time-step since we assume that the spatial domain is static.\n",
    "    return transformed_lons, transformed_lats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_frames(nc, lons, lats, animation_path):\n",
    "    \"\"\"\n",
    "    Make all the individual images which will comprise the final gif.\n",
    "    :param nc: the netCDF containing the data\n",
    "    :param lons: the 2D grid of longitudes transformed to the CRS != nc.lon.values\n",
    "    :param lats: the 2D grid of latitudes transformed to the CRS != nc.lat.values\n",
    "    :param animation_path: location in which to save the frames\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # one advantage of having built a Dataset of all the values we're going to animate is that I can now access\n",
    "    # the global min and max of all the data for our colorbar/contour levels\n",
    "    mslp_min = nc['msl_pressure:hPa'].min()\n",
    "    mslp_max = nc['msl_pressure:hPa'].max()\n",
    "    contour_levels = np.arange(np.floor(mslp_min), np.ceil(mslp_max), 2)\n",
    "    t2m_min = nc['t_2m:C'].min()\n",
    "    t2m_max = nc['t_2m:C'].max()\n",
    "\n",
    "    os.makedirs(animation_path, exist_ok=True)\n",
    "    for fle in glob.glob(os.path.join(animation_path, '*')):\n",
    "        os.remove(fle)  # remove all the previous animation bits and pieces\n",
    "\n",
    "    # TODO various bits of formatting can be done\n",
    "    # this will save a bunch of static images, which can of course be giffed\n",
    "    for t_step in range(len(nc.validdate)):\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(projection=CRS)\n",
    "        ax.set_extent([TOP_LEFT['lon'], BOTTOM_RIGHT['lon'], TOP_LEFT['lat'], BOTTOM_RIGHT['lat']])\n",
    "        nc_step = nc.isel(validdate=t_step)\n",
    "        cbar_map = ax.pcolor(lons, lats, nc_step['t_2m:C'], vmin=t2m_min, vmax=t2m_max)\n",
    "        contours = ax.contour(lons, lats, nc_step['msl_pressure:hPa'], levels=contour_levels, colors='black')\n",
    "        ax.clabel(contours, contours.levels[::5])\n",
    "        ax.coastlines()\n",
    "        plt.colorbar(cbar_map)\n",
    "        plt.title(pd.to_datetime(nc.validdate[t_step].values).strftime('%Y-%m-%d %Hh00'))\n",
    "        plt.savefig(os.path.join(animation_path, '{}_{}'.format(\n",
    "            str(t_step).zfill(4),  # include this so that order is preserved even if we cross into January\n",
    "            pd.to_datetime(nc.validdate[t_step].values).strftime('%Y-%m-%d %Hh00.png')\n",
    "        )))\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate(animation_path='animation'):\n",
    "    \"\"\"\n",
    "    Makes an animation of the temperature and pressure situation over the forecast time- and region specified.\n",
    "    All arguments other than the name of the animation directory are defined in the preamble.\n",
    "    :param animation_path:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    nc = make_nc()\n",
    "    lons, lats = transform_coords(nc)\n",
    "    make_frames(nc, lons, lats, animation_path)\n",
    "\n",
    "    # I'd have liked this to be a FuncAnimation with a slider for time control, but that seems complicated when\n",
    "    # including multiple functions on a single frame (as I do with pcolor and contour) so I cheat by making a GIF\n",
    "    # out of multiple images per https://bit.ly/3kRZOmB, and please check out my question https://bit.ly/3nwhNAT\n",
    "    # if you have suggestions on how to improve the readability of the GIF\"\n",
    "    img, *imgs = [Image.open(f) for f in sorted(glob.glob(os.path.join(animation_path, '*.png')))]\n",
    "    # 'duration' (below) means duration of each frame in milliseconds\n",
    "    img.save(fp=os.path.join(animation_path, 'animation.gif'), format='GIF', append_images=imgs, save_all=True,\n",
    "             duration=200, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "ename": "DataSourceError",
     "evalue": "polygons/SP.geojson: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDataSourceError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[190], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# get the geometries of the polygons for which we want time-series plots\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     geometries \u001b[38;5;241m=\u001b[39m \u001b[43mget_geometries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstandard_countries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSTANDARD_COUNTRIES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmanual_geometries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMANUAL_GEOMETRIES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSHOW_POLYGONS\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# for each of those polygons, write the time-series data and get a global min/max for each variable\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     mins \u001b[38;5;241m=\u001b[39m {var: np\u001b[38;5;241m.\u001b[39minf \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m TIMESERIES_VARS\u001b[38;5;241m.\u001b[39mvalues()}\n",
      "Cell \u001b[1;32mIn[177], line 22\u001b[0m, in \u001b[0;36mget_geometries\u001b[1;34m(standard_countries, manual_geometries, show)\u001b[0m\n\u001b[0;32m     20\u001b[0m         geometries_projected[country] \u001b[38;5;241m=\u001b[39m countries[countries[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNAME\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m country]\u001b[38;5;241m.\u001b[39mto_crs(CRS\u001b[38;5;241m.\u001b[39mproj4_init)\u001b[38;5;241m.\u001b[39mgeometry\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m country \u001b[38;5;129;01min\u001b[39;00m manual_geometries:\n\u001b[1;32m---> 22\u001b[0m     geometries[country] \u001b[38;5;241m=\u001b[39m \u001b[43mgpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmanual_geometries\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcountry\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgeometry\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m show:\n\u001b[0;32m     24\u001b[0m         geometries_projected[country] \u001b[38;5;241m=\u001b[39m gpd\u001b[38;5;241m.\u001b[39mread_file(manual_geometries[country])\u001b[38;5;241m.\u001b[39mto_crs(CRS\u001b[38;5;241m.\u001b[39mproj4_init)\u001b[38;5;241m.\u001b[39mgeometry\n",
      "File \u001b[1;32mc:\\Users\\axel.chepanski\\VigilumSP\\.venv\\Lib\\site-packages\\geopandas\\io\\file.py:294\u001b[0m, in \u001b[0;36m_read_file\u001b[1;34m(filename, bbox, mask, columns, rows, engine, **kwargs)\u001b[0m\n\u001b[0;32m    291\u001b[0m             from_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyogrio\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read_file_pyogrio\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiona\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mis_file_like(filename):\n",
      "File \u001b[1;32mc:\\Users\\axel.chepanski\\VigilumSP\\.venv\\Lib\\site-packages\\geopandas\\io\\file.py:547\u001b[0m, in \u001b[0;36m_read_file_pyogrio\u001b[1;34m(path_or_bytes, bbox, mask, rows, **kwargs)\u001b[0m\n\u001b[0;32m    538\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    539\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minclude_fields\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore_fields\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m keywords are deprecated, and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    540\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future release. You can use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m keyword \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    543\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m    544\u001b[0m     )\n\u001b[0;32m    545\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_fields\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 547\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpyogrio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\axel.chepanski\\VigilumSP\\.venv\\Lib\\site-packages\\pyogrio\\geopandas.py:261\u001b[0m, in \u001b[0;36mread_dataframe\u001b[1;34m(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, fid_as_index, use_arrow, on_invalid, arrow_to_pandas_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_arrow:\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;66;03m# For arrow, datetimes are read as is.\u001b[39;00m\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;66;03m# For numpy IO, datetimes are read as string values to preserve timezone info\u001b[39;00m\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# as numpy does not directly support timezones.\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatetime_as_string\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 261\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mread_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_geometry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgdal_force_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43msql\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msql_dialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_fids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfid_as_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_arrow:\n\u001b[0;32m    281\u001b[0m     meta, table \u001b[38;5;241m=\u001b[39m result\n",
      "File \u001b[1;32mc:\\Users\\axel.chepanski\\VigilumSP\\.venv\\Lib\\site-packages\\pyogrio\\raw.py:196\u001b[0m, in \u001b[0;36mread\u001b[1;34m(path_or_buffer, layer, encoding, columns, read_geometry, force_2d, skip_features, max_features, where, bbox, mask, fids, sql, sql_dialect, return_fids, datetime_as_string, **kwargs)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Read OGR data source into numpy arrays.\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \n\u001b[0;32m     58\u001b[0m \u001b[38;5;124;03mIMPORTANT: non-linear geometry types (e.g., MultiSurface) are converted\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    191\u001b[0m \n\u001b[0;32m    192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    194\u001b[0m dataset_kwargs \u001b[38;5;241m=\u001b[39m _preprocess_options_key_value(kwargs) \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m--> 196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mogr_read\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_vsi_path_or_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread_geometry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_geometry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_mask_to_wkb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43msql\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43msql_dialect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msql_dialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_fids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_fids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatetime_as_string\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatetime_as_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\axel.chepanski\\VigilumSP\\.venv\\Lib\\site-packages\\pyogrio\\_io.pyx:1239\u001b[0m, in \u001b[0;36mpyogrio._io.ogr_read\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\axel.chepanski\\VigilumSP\\.venv\\Lib\\site-packages\\pyogrio\\_io.pyx:219\u001b[0m, in \u001b[0;36mpyogrio._io.ogr_open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mDataSourceError\u001b[0m: polygons/SP.geojson: No such file or directory"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # get the geometries of the polygons for which we want time-series plots\n",
    "    geometries = get_geometries(\n",
    "        standard_countries=STANDARD_COUNTRIES,\n",
    "        manual_geometries=MANUAL_GEOMETRIES,\n",
    "        show=SHOW_POLYGONS\n",
    "    )\n",
    "\n",
    "    # for each of those polygons, write the time-series data and get a global min/max for each variable\n",
    "    mins = {var: np.inf for var in TIMESERIES_VARS.values()}\n",
    "    maxs = {var: -np.inf for var in TIMESERIES_VARS.values()}\n",
    "    for key in geometries:\n",
    "        mins, maxs = get_combined_data(geometries, key, mins, maxs)\n",
    "\n",
    "    # plot the time-series using these global values\n",
    "    for key in geometries:\n",
    "        plot_timeseries(key, mins, maxs)\n",
    "\n",
    "    # now make the GIF\n",
    "    animate('animation')  # all the other variables for this are controlled in the preamble"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
