{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import meteomatics.api as api\n",
    "import datetime as dt\n",
    "import shapely.geometry as sgeometry\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import geopandas as gpd\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import functools\n",
    "import glob\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINIÇÕES DE DIRETÓRIOS\n",
    "BASE_DIR = os.getcwd()\n",
    "DIR_DADOS = os.path.join(BASE_DIR, \"dados\")\n",
    "DIR_COUNTRIES = os.path.join(DIR_DADOS, \"ne_110m_admin_0_countries\")\n",
    "countries_shp_path = os.path.join(DIR_COUNTRIES, \"ne_110m_admin_0_countries.shp\")\n",
    "DIR_GPKG_SP = os.path.join(BASE_DIR, \"polygons\\SP_Malha_Preliminar_Distrito_2022.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GID_RM</th>\n",
       "      <th>RM</th>\n",
       "      <th>Lei</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>RM de Piracicaba</td>\n",
       "      <td>Lei Complementar Estadual 1360 de 24.08.2021</td>\n",
       "      <td>POLYGON ((-47.38827 -22.98354, -47.38797 -22.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>RM São José do Rio Preto</td>\n",
       "      <td>Lei Complementar Estadual 1359 de 24.08.2021</td>\n",
       "      <td>POLYGON ((-49.1531 -21.18245, -49.15309 -21.18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>RM de São Paulo</td>\n",
       "      <td>Lei Complementar 1.139 de 16.06.2011</td>\n",
       "      <td>POLYGON ((-47.0382 -23.83528, -47.03793 -23.83...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>RM de Campinas</td>\n",
       "      <td>Lei Complementar 870 de 19.06.2000 e Lei Compl...</td>\n",
       "      <td>POLYGON ((-47.08834 -23.05551, -47.08851 -23.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>RM de Sorocaba</td>\n",
       "      <td>Lei Complementar 1.241 de 20.06.2014 e Lei Com...</td>\n",
       "      <td>POLYGON ((-46.99506 -23.50697, -46.99469 -23.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>RM de Ribeirão Preto</td>\n",
       "      <td>Lei Complementar 1.290 de 06.07.2016</td>\n",
       "      <td>POLYGON ((-47.04466 -21.26713, -47.04465 -21.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>RM da Baixada Santista</td>\n",
       "      <td>Lei Complementar 815 de 30.07.1996</td>\n",
       "      <td>POLYGON ((-46.99128 -24.12202, -46.99068 -24.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>RM do Vale do Paraíba e Litoral Norte</td>\n",
       "      <td>Lei Complementar 1.166 de 09.01.2012</td>\n",
       "      <td>MULTIPOLYGON (((-45.52488 -23.8561, -45.52502 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>RM de Jundiai</td>\n",
       "      <td>Lei Complementar Estadual 1362 de 30.11.2021</td>\n",
       "      <td>POLYGON ((-46.86606 -23.07632, -46.86564 -23.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GID_RM                                     RM  \\\n",
       "0       7                       RM de Piracicaba   \n",
       "1       8               RM São José do Rio Preto   \n",
       "2       1                        RM de São Paulo   \n",
       "3       2                         RM de Campinas   \n",
       "4       5                         RM de Sorocaba   \n",
       "5       6                   RM de Ribeirão Preto   \n",
       "6       3                 RM da Baixada Santista   \n",
       "7       4  RM do Vale do Paraíba e Litoral Norte   \n",
       "8       9                          RM de Jundiai   \n",
       "\n",
       "                                                 Lei  \\\n",
       "0       Lei Complementar Estadual 1360 de 24.08.2021   \n",
       "1       Lei Complementar Estadual 1359 de 24.08.2021   \n",
       "2               Lei Complementar 1.139 de 16.06.2011   \n",
       "3  Lei Complementar 870 de 19.06.2000 e Lei Compl...   \n",
       "4  Lei Complementar 1.241 de 20.06.2014 e Lei Com...   \n",
       "5               Lei Complementar 1.290 de 06.07.2016   \n",
       "6                 Lei Complementar 815 de 30.07.1996   \n",
       "7               Lei Complementar 1.166 de 09.01.2012   \n",
       "8       Lei Complementar Estadual 1362 de 30.11.2021   \n",
       "\n",
       "                                            geometry  \n",
       "0  POLYGON ((-47.38827 -22.98354, -47.38797 -22.9...  \n",
       "1  POLYGON ((-49.1531 -21.18245, -49.15309 -21.18...  \n",
       "2  POLYGON ((-47.0382 -23.83528, -47.03793 -23.83...  \n",
       "3  POLYGON ((-47.08834 -23.05551, -47.08851 -23.0...  \n",
       "4  POLYGON ((-46.99506 -23.50697, -46.99469 -23.5...  \n",
       "5  POLYGON ((-47.04466 -21.26713, -47.04465 -21.2...  \n",
       "6  POLYGON ((-46.99128 -24.12202, -46.99068 -24.1...  \n",
       "7  MULTIPOLYGON (((-45.52488 -23.8561, -45.52502 ...  \n",
       "8  POLYGON ((-46.86606 -23.07632, -46.86564 -23.0...  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpd.read_file(DIR_DADOS+'\\\\regiao_metropolitana\\\\Regiões Metropolitanas.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o shapefile\n",
    "countries = gpd.read_file(countries_shp_path)\n",
    "paises = countries['NAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29    POLYGON ((-53.37366 -33.76838, -53.65054 -33.2...\n",
       "Name: geometry, dtype: geometry"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brasil = countries[countries['NAME'] == 'Brazil']\n",
    "brasil['geometry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometria = brasil['geometry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_polygon = gpd.read_file(DIR_GPKG_SP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CD_REGIAO</th>\n",
       "      <th>NM_REGIAO</th>\n",
       "      <th>CD_UF</th>\n",
       "      <th>NM_UF</th>\n",
       "      <th>CD_MUN</th>\n",
       "      <th>NM_MUN</th>\n",
       "      <th>CD_DIST</th>\n",
       "      <th>NM_DIST</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Sudeste</td>\n",
       "      <td>35</td>\n",
       "      <td>São Paulo</td>\n",
       "      <td>3500105</td>\n",
       "      <td>Adamantina</td>\n",
       "      <td>350010505</td>\n",
       "      <td>Adamantina</td>\n",
       "      <td>POLYGON ((-51.09557 -21.57029, -51.09617 -21.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Sudeste</td>\n",
       "      <td>35</td>\n",
       "      <td>São Paulo</td>\n",
       "      <td>3500204</td>\n",
       "      <td>Adolfo</td>\n",
       "      <td>350020405</td>\n",
       "      <td>Adolfo</td>\n",
       "      <td>POLYGON ((-49.61249 -21.2611, -49.61249 -21.26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Sudeste</td>\n",
       "      <td>35</td>\n",
       "      <td>São Paulo</td>\n",
       "      <td>3500303</td>\n",
       "      <td>Aguaí</td>\n",
       "      <td>350030305</td>\n",
       "      <td>Aguaí</td>\n",
       "      <td>POLYGON ((-47.01254 -22.00527, -47.01219 -22.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Sudeste</td>\n",
       "      <td>35</td>\n",
       "      <td>São Paulo</td>\n",
       "      <td>3500402</td>\n",
       "      <td>Águas da Prata</td>\n",
       "      <td>350040205</td>\n",
       "      <td>Águas da Prata</td>\n",
       "      <td>POLYGON ((-46.71875 -21.95837, -46.71878 -21.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Sudeste</td>\n",
       "      <td>35</td>\n",
       "      <td>São Paulo</td>\n",
       "      <td>3500402</td>\n",
       "      <td>Águas da Prata</td>\n",
       "      <td>350040210</td>\n",
       "      <td>São Roque da Fartura</td>\n",
       "      <td>POLYGON ((-46.73337 -21.87262, -46.73727 -21.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152</th>\n",
       "      <td>3</td>\n",
       "      <td>Sudeste</td>\n",
       "      <td>35</td>\n",
       "      <td>São Paulo</td>\n",
       "      <td>3557105</td>\n",
       "      <td>Votuporanga</td>\n",
       "      <td>355710515</td>\n",
       "      <td>Simonsen</td>\n",
       "      <td>POLYGON ((-49.85227 -20.41665, -49.85289 -20.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1153</th>\n",
       "      <td>3</td>\n",
       "      <td>Sudeste</td>\n",
       "      <td>35</td>\n",
       "      <td>São Paulo</td>\n",
       "      <td>3557154</td>\n",
       "      <td>Zacarias</td>\n",
       "      <td>355715405</td>\n",
       "      <td>Zacarias</td>\n",
       "      <td>POLYGON ((-49.98507 -21.05306, -49.98518 -21.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>3</td>\n",
       "      <td>Sudeste</td>\n",
       "      <td>35</td>\n",
       "      <td>São Paulo</td>\n",
       "      <td>3557204</td>\n",
       "      <td>Chavantes</td>\n",
       "      <td>355720405</td>\n",
       "      <td>Chavantes</td>\n",
       "      <td>POLYGON ((-49.73594 -23.03489, -49.74234 -23.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>3</td>\n",
       "      <td>Sudeste</td>\n",
       "      <td>35</td>\n",
       "      <td>São Paulo</td>\n",
       "      <td>3557204</td>\n",
       "      <td>Chavantes</td>\n",
       "      <td>355720415</td>\n",
       "      <td>Irapé</td>\n",
       "      <td>POLYGON ((-49.73594 -23.03489, -49.72426 -23.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>3</td>\n",
       "      <td>Sudeste</td>\n",
       "      <td>35</td>\n",
       "      <td>São Paulo</td>\n",
       "      <td>3557303</td>\n",
       "      <td>Estiva Gerbi</td>\n",
       "      <td>355730305</td>\n",
       "      <td>Estiva Gerbi</td>\n",
       "      <td>POLYGON ((-46.96444 -22.29548, -46.9661 -22.29...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1157 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     CD_REGIAO NM_REGIAO CD_UF      NM_UF   CD_MUN          NM_MUN    CD_DIST  \\\n",
       "0            3   Sudeste    35  São Paulo  3500105      Adamantina  350010505   \n",
       "1            3   Sudeste    35  São Paulo  3500204          Adolfo  350020405   \n",
       "2            3   Sudeste    35  São Paulo  3500303           Aguaí  350030305   \n",
       "3            3   Sudeste    35  São Paulo  3500402  Águas da Prata  350040205   \n",
       "4            3   Sudeste    35  São Paulo  3500402  Águas da Prata  350040210   \n",
       "...        ...       ...   ...        ...      ...             ...        ...   \n",
       "1152         3   Sudeste    35  São Paulo  3557105     Votuporanga  355710515   \n",
       "1153         3   Sudeste    35  São Paulo  3557154        Zacarias  355715405   \n",
       "1154         3   Sudeste    35  São Paulo  3557204       Chavantes  355720405   \n",
       "1155         3   Sudeste    35  São Paulo  3557204       Chavantes  355720415   \n",
       "1156         3   Sudeste    35  São Paulo  3557303    Estiva Gerbi  355730305   \n",
       "\n",
       "                   NM_DIST                                           geometry  \n",
       "0               Adamantina  POLYGON ((-51.09557 -21.57029, -51.09617 -21.5...  \n",
       "1                   Adolfo  POLYGON ((-49.61249 -21.2611, -49.61249 -21.26...  \n",
       "2                    Aguaí  POLYGON ((-47.01254 -22.00527, -47.01219 -22.0...  \n",
       "3           Águas da Prata  POLYGON ((-46.71875 -21.95837, -46.71878 -21.9...  \n",
       "4     São Roque da Fartura  POLYGON ((-46.73337 -21.87262, -46.73727 -21.8...  \n",
       "...                    ...                                                ...  \n",
       "1152              Simonsen  POLYGON ((-49.85227 -20.41665, -49.85289 -20.4...  \n",
       "1153              Zacarias  POLYGON ((-49.98507 -21.05306, -49.98518 -21.0...  \n",
       "1154             Chavantes  POLYGON ((-49.73594 -23.03489, -49.74234 -23.0...  \n",
       "1155                 Irapé  POLYGON ((-49.73594 -23.03489, -49.72426 -23.0...  \n",
       "1156          Estiva Gerbi  POLYGON ((-46.96444 -22.29548, -46.9661 -22.29...  \n",
       "\n",
       "[1157 rows x 9 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  CONFIGURATION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. What countries/regions do you want in your time-series?\n",
    "# These are provided in two arguments. 'STANDARD_COUNTRIES' is a list of strings which are the names of polygons in\n",
    "# geopandas' \"naturalearth_lowres\" built-in dataset. 'MANUAL_GEOMETRIES' is a dictionary whose keys are the names you\n",
    "# want to give to your user-created polygons, and whose keys are the paths to geojson files containing those polygons.\n",
    "# I advise that, if you make changes to this, you check that they have been interpreted correctly by switching\n",
    "# 'SHOW_POLYGONS' to True. This will show them in your chosen map projection. Speaking of which...\n",
    "STANDARD_COUNTRIES = ['Brazil']\n",
    "MANUAL_GEOMETRIES = {'São Paulo': DIR_GPKG_SP}\n",
    "SHOW_POLYGONS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. What map projection do you want to show your animated data in?\n",
    "# Change entries in the dictionaries to change the bounding box for your animations. This will make a rectangular plot\n",
    "# regardless of the projection. The dictionary values are then used to define the map projection. You can choose from\n",
    "# any cartopy projection, but be aware that a) some projections take different arguments to define the transformations\n",
    "# and b) some projections will not fill the rectangle defined by the dictionaries in certain parts of the globe (for\n",
    "# instance, if you use AzimuthalEquidistant in Europe you'll see the fetched data get squeezed at the top of the plot).\n",
    "TOP_LEFT = {\n",
    "    'lat': 66.211199,\n",
    "    'lon': -32.364446\n",
    "}\n",
    "BOTTOM_RIGHT = {\n",
    "    'lat': 35.526388,\n",
    "    'lon': 32\n",
    "}\n",
    "# CRS = ccrs.AzimuthalEquidistant(central_longitude=(TOP_LEFT['lon']+BOTTOM_RIGHT['lon'])/2,\n",
    "#                                 central_latitude=(TOP_LEFT['lat']+BOTTOM_RIGHT['lat'])/2)\n",
    "CRS = ccrs.Mercator(central_longitude=(TOP_LEFT['lon']+BOTTOM_RIGHT['lon'])/2,\n",
    "                    min_latitude=BOTTOM_RIGHT['lat'],\n",
    "                    max_latitude=TOP_LEFT['lat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. What variables are you interested in?\n",
    "# Currently this choice only affects time-series plots: the variables used in the animation are static (temperature and\n",
    "# pressure, see animate <== make_nc). Keys should be the names you want on your axis labels, and should feature exactly\n",
    "# one set of parentheses enclosing the units; values are the names of the corresponding meteomatics strings.\n",
    "TIMESERIES_VARS = {\n",
    "    'Temperature (C)': 't_2m:C',\n",
    "    '90m Wind Speed (m/s)': 'wind_speed_90m:ms',\n",
    "    'Global Radiation (W/m2)': 'global_rad:W'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. How do you want to define the baseline (climatology) for your time-series?\n",
    "# 15 years is apparently standard for energy industry; 30 years is standard in academic texts. Climatologies are all\n",
    "# interpolated to 1hrly resolution, but I recommend sub-6hrly for data acquisition, since 0600 and 1800 vary between\n",
    "# being daytime- and nighttime values throughout the year. Also note that the API assumes all times are UTC, so a\n",
    "# smaller time-step is better for translating to other time-zones). Don't let your climatology get too out of date!\n",
    "CLIMATOLOGY_START_YEAR = 2005\n",
    "CLIMATOLOGY_STOP_YEAR = 2020\n",
    "CLIMATOLOGY_STEP = dt.timedelta(hours=3)\n",
    "\n",
    "# IMPORTANT! Climatologies are obtained once per new run environment and then imported from .csv in order to reduce\n",
    "# runtime. The script is not clever enough to know whether you have changed TIMESERIES_VARS or the period to be covered\n",
    "# by climatology, so make sure you force the recalculation of climatology whenever changes are made.\n",
    "REBUILD_CLIMATOLOGY = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Plot formatting.\n",
    "#   What period do you want your output (time-series and animations) to cover?\n",
    "#   What temporal resolution do you want for them both (set separately)?\n",
    "#   What spatial resolution do you want in your\n",
    "# Note that the animation may fail if you set the time-step very high\n",
    "LEAD_TIME_DAYS = 28\n",
    "START_TIME = dt.datetime.combine(dt.datetime.now().date(), dt.time(12))  # starts at 12pm on the day called\n",
    "T_SERIES_LEAD = dt.timedelta(days=LEAD_TIME_DAYS)\n",
    "T_SERIES_STEP = dt.timedelta(hours=1)\n",
    "ANIMATION_STEP = dt.timedelta(hours=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API access\n",
    "USERNAME = os.getenv('USERNAME_WEATHER_API')\n",
    "PASSWORD = os.getenv('PASSWORD_WEATHER_API')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(func):\n",
    "    \"\"\"Print the runtime of the decorated function\"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper_timer(*args, **kwargs):\n",
    "        t_start_time = time.perf_counter()    # 1\n",
    "        value = func(*args, **kwargs)\n",
    "        end_time = time.perf_counter()      # 2\n",
    "        run_time = end_time - t_start_time    # 3\n",
    "        print(f\"Finished {func.__name__!r} in {run_time:.4f} secs\")\n",
    "        return value\n",
    "    return wrapper_timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_geometries(geometries):  # TODO could add an option to show overlap\n",
    "    \"\"\"\n",
    "    Plots the projected polygons\n",
    "    :param geometries: shapely.[Multi]Polygon objects corresponding to polygons projected into CRS (defined in header)\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(projection=CRS)\n",
    "    ax.stock_img()\n",
    "    ax.set_extent([TOP_LEFT['lon'], BOTTOM_RIGHT['lon'], TOP_LEFT['lat'], BOTTOM_RIGHT['lat']])\n",
    "    for geom in geometries:\n",
    "        geometries[geom].plot(ax=ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geometries(standard_countries, manual_geometries, show=False):\n",
    "    \"\"\"\n",
    "    Uses geopandas' built-in countries for those polygons which correspond to a whole country; otherwise can use custom\n",
    "    geometries provided in GeoJSON format (with corresponding user-defined polygon names: see parameter docstrings).\n",
    "    :param standard_countries: list of country names as they appear in naturalearth_lowres\n",
    "    :param manual_geometries: dictionary of:\n",
    "                polygon names : GeoJSON paths\n",
    "    :param show: Bool, if True will illustrate location and shape of polygons on given map projection\n",
    "    :return: dictionary of:\n",
    "                polygon names (standard_countries + manual_geometries.keys) : shapely.[Multi]Polygon\n",
    "    \"\"\"\n",
    "    # Utilize o novo método para carregar os dados naturalearth_lowres\n",
    "    countries = gpd.read_file(countries_shp_path)\n",
    "\n",
    "    geometries = {}\n",
    "    geometries_projected = {}  # will be filled with geometries in chosen projection if show is True\n",
    "    for country in standard_countries:\n",
    "        geometries[country] = countries[countries['NAME'] == country].geometry.values[0]  # Use a coluna correta aqui\n",
    "        if show:\n",
    "            geometries_projected[country] = countries[countries['NAME'] == country].to_crs(CRS.proj4_init).geometry\n",
    "    for country in manual_geometries:\n",
    "        geometries[country] = gpd.read_file(manual_geometries[country]).geometry.values[0]\n",
    "        if show:\n",
    "            geometries_projected[country] = gpd.read_file(manual_geometries[country]).to_crs(CRS.proj4_init).geometry\n",
    "    if show:\n",
    "        show_geometries(geometries_projected)\n",
    "    return geometries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tuple_list(polygon):\n",
    "    \"\"\"\n",
    "    Subroutine of make_tuple_lists. Argument will always be a Polygon, since parent function loops over MultiPolygon\n",
    "    :param polygon: shapely.Polygon\n",
    "    :return: tuple list\n",
    "    \"\"\"\n",
    "    tuple_list = []\n",
    "    lons, lats = polygon.exterior.coords.xy\n",
    "    for i in range(len(lons)):\n",
    "        tuple_list.append((lats[i], lons[i]))\n",
    "    return tuple_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tuple_lists(geometry):\n",
    "    \"\"\"\n",
    "    Prepare the tuple list argument for API query from polygon data\n",
    "    :param geometry: shapely.[Multi]Polygon\n",
    "    :return: tuple lists (tuple list for each Polygon)\n",
    "    \"\"\"\n",
    "    retval = []\n",
    "    if type(geometry) is sgeometry.multipolygon.MultiPolygon:\n",
    "        for polygon in geometry:  # this is the correct way to access Polygons within a MultiPolygon...\n",
    "            retval.append(make_tuple_list(polygon))\n",
    "    elif type(geometry) is sgeometry.polygon.Polygon:\n",
    "        retval.append(make_tuple_list(geometry))  # ...but you can't loop over a single Polygon (which is dumb)\n",
    "    else:\n",
    "        # code should be written such that only [Multi]Polygons make it here; if imported modules change, will raise\n",
    "        raise TypeError('geometries should contain only polygons or multipolygons')\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_climatology(geometries, location):\n",
    "    \"\"\"\n",
    "    This only looks hefty because of the if/else block, plus the fact that I've extended API calls over multiple lines.\n",
    "    Makes a climatology dataframe by joining years together sequentially.\n",
    "    :param geometries: dictionary of:\n",
    "                locations : shapely.[Multi]Polygons\n",
    "    :param location: the name of the location. passed because desired for name of written .csv\n",
    "    \"\"\"\n",
    "    geometry = geometries[location]\n",
    "    tuple_lists = make_tuple_lists(geometry)\n",
    "    print('Fetching climatology for {}.'.format(location))\n",
    "    # had to break this down because of 300s timeout. 3hrly still seems reasonable and a bit faster; could go 1hrly now\n",
    "    for year in range(CLIMATOLOGY_START_YEAR, CLIMATOLOGY_STOP_YEAR):\n",
    "        if year == CLIMATOLOGY_START_YEAR:\n",
    "            df = api.query_polygon(\n",
    "                latlon_tuple_lists=tuple_lists,\n",
    "                startdate=dt.datetime(year, 1, 1),\n",
    "                enddate=dt.datetime(year, 12, 31, int(24 - CLIMATOLOGY_STEP.seconds/3600.)),\n",
    "                interval=CLIMATOLOGY_STEP,\n",
    "                parameters=TIMESERIES_VARS.values(),\n",
    "                aggregation=['mean'],\n",
    "                username=USERNAME,\n",
    "                password=PASSWORD,\n",
    "                operator='U',  # in case of MultiPolygons\n",
    "                model='ecmwf-era5',\n",
    "                polygon_sampling='adaptive_grid'\n",
    "            )\n",
    "        else:\n",
    "            df = df.append(\n",
    "                api.query_polygon(\n",
    "                    latlon_tuple_lists=tuple_lists,\n",
    "                    startdate=dt.datetime(year, 1, 1),\n",
    "                    enddate=dt.datetime(year, 12, 31, int(24 - CLIMATOLOGY_STEP.seconds/3600.)),\n",
    "                    interval=CLIMATOLOGY_STEP,\n",
    "                    parameters=TIMESERIES_VARS.values(),\n",
    "                    aggregation=['mean'],\n",
    "                    username=USERNAME,\n",
    "                    password=PASSWORD,\n",
    "                    operator='U',  # in case of MultiPolygons\n",
    "                    model='ecmwf-era5',\n",
    "                    polygon_sampling='adaptive_grid'\n",
    "                )\n",
    "            )\n",
    "    # Query returns MultiIndexed DataFrame, but outer index is polygon1 even if geometry is a MultiPolygon (because\n",
    "    # of the union operation). To make this easier to work with, I cross-section to get rid of the outer index level.\n",
    "    # To future-proof, make sure that there is only one outer index - if so, remove it.\n",
    "    assert len(df.index.get_level_values(0).unique()) == 1\n",
    "    df = df.xs(key='polygon1')\n",
    "    df = df.resample('H').interpolate(method='cubic')\n",
    "    climatology = df.groupby([df.index.dayofyear, df.index.time]).mean()\n",
    "    climatology.index.names = ['doy', 'time']\n",
    "    climatology.to_csv(os.path.join('climatologies', '{}.csv'.format(location)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def get_polygon_climatology(geometries, location):  # TODO could save climatology locally and only fetch if required\n",
    "    if os.path.exists(os.path.join('climatologies', '{}.csv'.format(location))) and not REBUILD_CLIMATOLOGY:\n",
    "        print('Climatology for {} exists: reading climatology'.format(location))\n",
    "    else:\n",
    "        print('New climatology required for {}.'.format(location))\n",
    "        os.makedirs('climatologies', exist_ok=True)\n",
    "        write_climatology(geometries, location)\n",
    "    df = pd.read_csv(os.path.join('climatologies', '{}.csv'.format(location)), index_col=[0, 1])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timer\n",
    "def get_polygon_data(geometry):  # TODO could make parameters variable\n",
    "    \"\"\"\n",
    "    Gets mean weather data for 2m temperature and 90m wind-speed (currently static) within polygon defined by geometry\n",
    "    :param geometry: shapely.[Multi]Polygon\n",
    "    :return: pandas.DataFrame of spatial mean time-series data for the polygon; time-series parameters defined in header\n",
    "    \"\"\"\n",
    "    tuple_lists = make_tuple_lists(geometry)\n",
    "    print('Fetching time-series data for the next {} days.'.format(LEAD_TIME_DAYS))\n",
    "    df = api.query_polygon(\n",
    "        latlon_tuple_lists=tuple_lists,\n",
    "        startdate=START_TIME,\n",
    "        enddate=START_TIME + T_SERIES_LEAD,\n",
    "        interval=T_SERIES_STEP,\n",
    "        parameters=TIMESERIES_VARS.values(),\n",
    "        aggregation=['mean'],\n",
    "        username=USERNAME,\n",
    "        password=PASSWORD,\n",
    "        operator='U',  # not necessary for the single polygon, but doesn't break and is necessary for multi-polygons\n",
    "        model='ecmwf-vareps',\n",
    "        polygon_sampling='adaptive_grid'\n",
    "    )\n",
    "    # Query returns MultiIndexed DataFrame, but outer index is polygon1 even if geometry is a MultiPolygon (because\n",
    "    # of the union operation). To make this easier to work with, I cross-section to get rid of the outer index level.\n",
    "    # To future-proof, make sure that there is only one outer index - if so, remove it.\n",
    "    assert len(df.index.get_level_values(0).unique()) == 1\n",
    "    df = df.xs(key='polygon1')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(fcst, clim):\n",
    "    \"\"\"\n",
    "    Joins the forecast and the corresponding climatology data together into one DataFrame.\n",
    "    :param fcst:\n",
    "    :param clim:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    relevant_clim = pd.DataFrame(index=fcst.index, columns=fcst.columns)\n",
    "    for i in range(len(fcst)):\n",
    "        tstamp = fcst.index[i]\n",
    "        relevant_clim.loc[tstamp] = clim.loc[(tstamp.dayofyear, tstamp.time().strftime('%H:%M:%S'))]\n",
    "    return pd.merge(fcst, relevant_clim, left_index=True, right_index=True, suffixes=('_forecast', '_climatology'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_data(geometries, key, mins, maxs):\n",
    "    \"\"\"\n",
    "    Reads the climatology and forecast data, combines them, writes the result to file and updates the min/max dicts.\n",
    "    :param geometries: dict of shapely.[Multi]Polygons\n",
    "    :param key: the country/region of interest\n",
    "    :param mins: dict of minimum values for variables across all regions\n",
    "    :param maxs: dict of maximum values for variables across all regions\n",
    "    :return: (updated) dicts of minimum/maximum values for variables across all regions\n",
    "    \"\"\"\n",
    "    clim = get_polygon_climatology(geometries, key)  # pass the key in order to name/lookup the file\n",
    "    fcst = get_polygon_data(geometries[key])\n",
    "    combined = combine(fcst, clim)\n",
    "    for var in fcst.columns:\n",
    "        mins[var] = min(mins[var], min(fcst[var]))\n",
    "        maxs[var] = max(maxs[var], max(fcst[var]))\n",
    "    os.makedirs('time-series', exist_ok=True)\n",
    "    combined.to_csv(os.path.join('time-series', 'time-series data {}.csv'.format(key)))\n",
    "    return mins, maxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_timeseries(title, mins, maxs):\n",
    "    \"\"\"\n",
    "    Generic function for plotting time-series data and anomaly from climatology of any variable (specified in header)\n",
    "    :param title: the name of the region being plotted: will be the title of the plot and the name of the file\n",
    "    :param mins: dictionary of global minimum values corresponding to each variable\n",
    "    :param maxs: dictionary of global maximum values corresponding to each variable\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(os.path.join('time-series', 'time-series data {}.csv'.format(title)), index_col=0, parse_dates=True)\n",
    "    for var in TIMESERIES_VARS:\n",
    "        try:\n",
    "            assert '(' in var\n",
    "        except AssertionError:\n",
    "            raise ValueError('Make sure the TIMESERIES_VARS dict is formatted correctly')\n",
    "        mm_string = TIMESERIES_VARS[var]\n",
    "        unit_index = len(var.split('(')[0])\n",
    "        vmin = mins[mm_string] - (maxs[mm_string] - mins[mm_string]) * 0.1\n",
    "        vmax = maxs[mm_string] + (maxs[mm_string] - mins[mm_string]) * 0.1\n",
    "        f, (ax1, ax2) = plt.subplots(2, sharex='col', figsize=(12, 7))\n",
    "        climatology_years = CLIMATOLOGY_STOP_YEAR - CLIMATOLOGY_START_YEAR\n",
    "        df['{}_climatology'.format(mm_string)].plot(label='{}-year mean'.format(climatology_years), ax=ax1, c='b')\n",
    "        df['{}_forecast'.format(mm_string)].plot(label='forecast', ax=ax1, ylim=[vmin, vmax], c='k')\n",
    "        pd.Series(np.zeros(len(df.index)), index=df.index).plot(ax=ax2, linestyle='dashed', c='b')\n",
    "        (df['{}_forecast'.format(mm_string)] - df['{}_climatology'.format(mm_string)]).plot(ax=ax2, c='k')\n",
    "        # plot formatting options\n",
    "        f.suptitle('{}'.format(title))\n",
    "        ax1.set_ylabel(var)\n",
    "        ax1.legend()\n",
    "        ax2.set_ylabel(var[:unit_index]+'Anomaly '+var[unit_index:])\n",
    "        ax1.grid(True)\n",
    "        ax2.grid(True)\n",
    "        f.tight_layout()\n",
    "        os.makedirs(os.path.join('time-series', '{}'.format(var.replace('/', ''))), exist_ok=True)\n",
    "        plt.savefig(os.path.join('time-series', '{}'.format(var.replace('/', '')), '{}.png'.format(title)))\n",
    "        plt.close(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ timer\n",
    "def make_nc():\n",
    "    \"\"\"\n",
    "    A single API call to retrieve all this data is too big, so I loop over days in our forecast range and stack the\n",
    "    grids together. Using query_grid_timeseries is still desirable for this, since it can be manipulated into an\n",
    "    xarray.Dataset very easily.\n",
    "    :return: xarray.Dataset containing 2m temperature and MSLP\n",
    "    \"\"\"\n",
    "    for day in range(LEAD_TIME_DAYS):\n",
    "        query_start = START_TIME + dt.timedelta(days=day)\n",
    "        df = api.query_grid_timeseries(\n",
    "            startdate=query_start,\n",
    "            enddate=query_start + dt.timedelta(hours=23), # ensures we don't double-count days\n",
    "            interval=ANIMATION_STEP,\n",
    "            parameters=['t_2m:C', 'msl_pressure:hPa'],\n",
    "            lat_N=np.ceil(TOP_LEFT['lat'] + 1),\n",
    "            lon_W=np.floor(TOP_LEFT['lon'] - 1),\n",
    "            lat_S=np.floor(BOTTOM_RIGHT['lat'] - 1),\n",
    "            lon_E=np.ceil(BOTTOM_RIGHT['lon'] + 1),\n",
    "            res_lon=0.5,\n",
    "            res_lat=0.5,\n",
    "            username=USERNAME,\n",
    "            password=PASSWORD,\n",
    "            model='ecmwf-vareps'\n",
    "        )\n",
    "        if day == 0:\n",
    "            nc = df.to_xarray()\n",
    "        else:\n",
    "            tmp = df.to_xarray()\n",
    "            nc = xr.concat([nc, tmp], dim='validdate')\n",
    "    try:\n",
    "        # TODO this currently fails because Meteomatics data is float64 and to_netcdf wants float32\n",
    "        nc.to_netcdf('run_{}'.format(dt.datetime.now().strftime('%Y-%m-%d_%Hh00.nc')))\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_coords(nc):\n",
    "    \"\"\"\n",
    "    I found the suggested method of adding a transorm=CRS keyword argument to plotting functions not to work with\n",
    "    e.g. Azimuthal equidistant. Perhaps this is something to do with pcolor. Anyway, here's my solution: define a\n",
    "    2D latitude and longitude variable which corresponds to the data variables; transform those and return them.\n",
    "    :param nc: xarray dataset containing latitude and longitude coordinates\n",
    "    :return: longitude and latitudes transformed to your chosen CRS\n",
    "    \"\"\"\n",
    "    # We first need latitude- and longitude arrays of equal size. Since we may be working with a map which has different\n",
    "    # x- and y-dimensions, and also since we may not be mapping to a rectilinear coordinate system, we first have to\n",
    "    # make 2D arrays of each\n",
    "    meshed_lon, meshed_lat = np.meshgrid(nc.lon.values, nc.lat.values)\n",
    "    # We can then transform these to our target CRS. The source CRS is PlateCarree() i.e. PlateCarree with default args\n",
    "    # i.e. quadratic grid, as this is the coordinate system returned from the Meteomatics API.\n",
    "    transformed_output = CRS.transform_points(ccrs.PlateCarree(), meshed_lon, meshed_lat)\n",
    "    # This gives us a 3D array of x, y, z; the latter will, for our purposes, be identically 0. We can subset this as\n",
    "    transformed_lons = transformed_output[:, :, 0]\n",
    "    transformed_lats = transformed_output[:, :, 1]\n",
    "    # Note that these are again 2D (because the same latitudes are not used for all longitudes and vice versa in a\n",
    "    # non-rectangular projection) and hence cannot be used as coordinates in an xarray.Dataset, but can be added as\n",
    "    # additional data variables if you like.\n",
    "    # This produces lat/lon fields for each element of the weather variables which allow them to be plotted in this\n",
    "    # projection. The process needn't be repeated for each time-step since we assume that the spatial domain is static.\n",
    "    return transformed_lons, transformed_lats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_frames(nc, lons, lats, animation_path):\n",
    "    \"\"\"\n",
    "    Make all the individual images which will comprise the final gif.\n",
    "    :param nc: the netCDF containing the data\n",
    "    :param lons: the 2D grid of longitudes transformed to the CRS != nc.lon.values\n",
    "    :param lats: the 2D grid of latitudes transformed to the CRS != nc.lat.values\n",
    "    :param animation_path: location in which to save the frames\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # one advantage of having built a Dataset of all the values we're going to animate is that I can now access\n",
    "    # the global min and max of all the data for our colorbar/contour levels\n",
    "    mslp_min = nc['msl_pressure:hPa'].min()\n",
    "    mslp_max = nc['msl_pressure:hPa'].max()\n",
    "    contour_levels = np.arange(np.floor(mslp_min), np.ceil(mslp_max), 2)\n",
    "    t2m_min = nc['t_2m:C'].min()\n",
    "    t2m_max = nc['t_2m:C'].max()\n",
    "\n",
    "    os.makedirs(animation_path, exist_ok=True)\n",
    "    for fle in glob.glob(os.path.join(animation_path, '*')):\n",
    "        os.remove(fle)  # remove all the previous animation bits and pieces\n",
    "\n",
    "    # TODO various bits of formatting can be done\n",
    "    # this will save a bunch of static images, which can of course be giffed\n",
    "    for t_step in range(len(nc.validdate)):\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(projection=CRS)\n",
    "        ax.set_extent([TOP_LEFT['lon'], BOTTOM_RIGHT['lon'], TOP_LEFT['lat'], BOTTOM_RIGHT['lat']])\n",
    "        nc_step = nc.isel(validdate=t_step)\n",
    "        cbar_map = ax.pcolor(lons, lats, nc_step['t_2m:C'], vmin=t2m_min, vmax=t2m_max)\n",
    "        contours = ax.contour(lons, lats, nc_step['msl_pressure:hPa'], levels=contour_levels, colors='black')\n",
    "        ax.clabel(contours, contours.levels[::5])\n",
    "        ax.coastlines()\n",
    "        plt.colorbar(cbar_map)\n",
    "        plt.title(pd.to_datetime(nc.validdate[t_step].values).strftime('%Y-%m-%d %Hh00'))\n",
    "        plt.savefig(os.path.join(animation_path, '{}_{}'.format(\n",
    "            str(t_step).zfill(4),  # include this so that order is preserved even if we cross into January\n",
    "            pd.to_datetime(nc.validdate[t_step].values).strftime('%Y-%m-%d %Hh00.png')\n",
    "        )))\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate(animation_path='animation'):\n",
    "    \"\"\"\n",
    "    Makes an animation of the temperature and pressure situation over the forecast time- and region specified.\n",
    "    All arguments other than the name of the animation directory are defined in the preamble.\n",
    "    :param animation_path:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    nc = make_nc()\n",
    "    lons, lats = transform_coords(nc)\n",
    "    make_frames(nc, lons, lats, animation_path)\n",
    "\n",
    "    # I'd have liked this to be a FuncAnimation with a slider for time control, but that seems complicated when\n",
    "    # including multiple functions on a single frame (as I do with pcolor and contour) so I cheat by making a GIF\n",
    "    # out of multiple images per https://bit.ly/3kRZOmB, and please check out my question https://bit.ly/3nwhNAT\n",
    "    # if you have suggestions on how to improve the readability of the GIF\"\n",
    "    img, *imgs = [Image.open(f) for f in sorted(glob.glob(os.path.join(animation_path, '*.png')))]\n",
    "    # 'duration' (below) means duration of each frame in milliseconds\n",
    "    img.save(fp=os.path.join(animation_path, 'animation.gif'), format='GIF', append_images=imgs, save_all=True,\n",
    "             duration=200, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New climatology required for Brazil.\n",
      "Fetching climatology for Brazil.\n"
     ]
    },
    {
     "ename": "Forbidden",
     "evalue": "Request with valid date 2005-01-01T00:00:00Z requires data access before 2024-08-25T00:00:00Z, which is not granted with this subscription type (e.g. trial). The valid time period for this account type starts at 2024-08-25T00:00:00Z and ends at 2026-08-26T00:00:00Z. Please contact sales@meteomatics.com and we are happy to provide an extended trial or an upgrade of your account.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mForbidden\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m maxs \u001b[38;5;241m=\u001b[39m {var: \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m TIMESERIES_VARS\u001b[38;5;241m.\u001b[39mvalues()}\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m geometries:\n\u001b[1;32m---> 13\u001b[0m     mins, maxs \u001b[38;5;241m=\u001b[39m \u001b[43mget_combined_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeometries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# plot the time-series using these global values\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m geometries:\n",
      "Cell \u001b[1;32mIn[69], line 10\u001b[0m, in \u001b[0;36mget_combined_data\u001b[1;34m(geometries, key, mins, maxs)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_combined_data\u001b[39m(geometries, key, mins, maxs):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    Reads the climatology and forecast data, combines them, writes the result to file and updates the min/max dicts.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    :param geometries: dict of shapely.[Multi]Polygons\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m    :return: (updated) dicts of minimum/maximum values for variables across all regions\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m     clim \u001b[38;5;241m=\u001b[39m \u001b[43mget_polygon_climatology\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeometries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pass the key in order to name/lookup the file\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     fcst \u001b[38;5;241m=\u001b[39m get_polygon_data(geometries[key])\n\u001b[0;32m     12\u001b[0m     combined \u001b[38;5;241m=\u001b[39m combine(fcst, clim)\n",
      "Cell \u001b[1;32mIn[60], line 6\u001b[0m, in \u001b[0;36mtimer.<locals>.wrapper_timer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper_timer\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m      5\u001b[0m     t_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()    \u001b[38;5;66;03m# 1\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()      \u001b[38;5;66;03m# 2\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     run_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m t_start_time    \u001b[38;5;66;03m# 3\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[66], line 8\u001b[0m, in \u001b[0;36mget_polygon_climatology\u001b[1;34m(geometries, location)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNew climatology required for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(location))\n\u001b[0;32m      7\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclimatologies\u001b[39m\u001b[38;5;124m'\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 8\u001b[0m     \u001b[43mwrite_climatology\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeometries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclimatologies\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(location)), index_col\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "Cell \u001b[1;32mIn[65], line 15\u001b[0m, in \u001b[0;36mwrite_climatology\u001b[1;34m(geometries, location)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m year \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(CLIMATOLOGY_START_YEAR, CLIMATOLOGY_STOP_YEAR):\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m year \u001b[38;5;241m==\u001b[39m CLIMATOLOGY_START_YEAR:\n\u001b[1;32m---> 15\u001b[0m         df \u001b[38;5;241m=\u001b[39m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_polygon\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlatlon_tuple_lists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtuple_lists\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstartdate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43menddate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m31\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mCLIMATOLOGY_STEP\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseconds\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m3600.\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m            \u001b[49m\u001b[43minterval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCLIMATOLOGY_STEP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTIMESERIES_VARS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m            \u001b[49m\u001b[43maggregation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m            \u001b[49m\u001b[43musername\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUSERNAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPASSWORD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m            \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mU\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# in case of MultiPolygons\u001b[39;49;00m\n\u001b[0;32m     25\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mecmwf-era5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpolygon_sampling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43madaptive_grid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     29\u001b[0m         df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m     30\u001b[0m             api\u001b[38;5;241m.\u001b[39mquery_polygon(\n\u001b[0;32m     31\u001b[0m                 latlon_tuple_lists\u001b[38;5;241m=\u001b[39mtuple_lists,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m             )\n\u001b[0;32m     43\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\OneDrive\\Área de Trabalho\\Enterprise Challenge\\VigilumSP\\.venv\\Lib\\site-packages\\meteomatics\\api.py:449\u001b[0m, in \u001b[0;36mquery_polygon\u001b[1;34m(latlon_tuple_lists, startdate, enddate, interval, parameters, aggregation, username, password, operator, model, ens_select, interp_select, on_invalid, api_base_url, request_type, cluster_select, **kwargs)\u001b[0m\n\u001b[0;32m    438\u001b[0m coordinates \u001b[38;5;241m=\u001b[39m build_coordinates_str_for_polygon(latlon_tuple_lists, aggregation, operator)\n\u001b[0;32m    439\u001b[0m url \u001b[38;5;241m=\u001b[39m POLYGON_TEMPLATE\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    440\u001b[0m     api_base_url\u001b[38;5;241m=\u001b[39mapi_base_url,\n\u001b[0;32m    441\u001b[0m     coordinates_aggregation\u001b[38;5;241m=\u001b[39mcoordinates,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    446\u001b[0m     urlParams\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k, v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m url_params_dict\u001b[38;5;241m.\u001b[39mitems()])\n\u001b[0;32m    447\u001b[0m )\n\u001b[1;32m--> 449\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mquery_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musername\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    450\u001b[0m df \u001b[38;5;241m=\u001b[39m convert_polygon_response_to_df(response\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\OneDrive\\Área de Trabalho\\Enterprise Challenge\\VigilumSP\\.venv\\Lib\\site-packages\\meteomatics\\api.py:119\u001b[0m, in \u001b[0;36mquery_api\u001b[1;34m(url, username, password, request_type, timeout_seconds, headers)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m requests\u001b[38;5;241m.\u001b[39mcodes\u001b[38;5;241m.\u001b[39mok:\n\u001b[0;32m    118\u001b[0m     exc \u001b[38;5;241m=\u001b[39m API_EXCEPTIONS[response\u001b[38;5;241m.\u001b[39mstatus_code]\n\u001b[1;32m--> 119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc(response\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[1;31mForbidden\u001b[0m: Request with valid date 2005-01-01T00:00:00Z requires data access before 2024-08-25T00:00:00Z, which is not granted with this subscription type (e.g. trial). The valid time period for this account type starts at 2024-08-25T00:00:00Z and ends at 2026-08-26T00:00:00Z. Please contact sales@meteomatics.com and we are happy to provide an extended trial or an upgrade of your account."
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # get the geometries of the polygons for which we want time-series plots\n",
    "    geometries = get_geometries(\n",
    "        standard_countries=STANDARD_COUNTRIES,\n",
    "        manual_geometries=MANUAL_GEOMETRIES,\n",
    "        show=SHOW_POLYGONS\n",
    "    )\n",
    "\n",
    "    # for each of those polygons, write the time-series data and get a global min/max for each variable\n",
    "    mins = {var: np.inf for var in TIMESERIES_VARS.values()}\n",
    "    maxs = {var: -np.inf for var in TIMESERIES_VARS.values()}\n",
    "    for key in geometries:\n",
    "        mins, maxs = get_combined_data(geometries, key, mins, maxs)\n",
    "\n",
    "    # plot the time-series using these global values\n",
    "    for key in geometries:\n",
    "        plot_timeseries(key, mins, maxs)\n",
    "\n",
    "    # now make the GIF\n",
    "    animate('animation')  # all the other variables for this are controlled in the preamble"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
